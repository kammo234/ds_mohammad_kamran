{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Notebook 1 â€” Data Loading, Cleaning, EDA, and Sentiment Join\n\n**Candidate:** Mohammad_Kamran  \n**Date:** 2025-08-29\n\nThis notebook:\n1. Downloads the datasets from Google Drive.  \n2. Loads + cleans Hyperliquid trader data.  \n3. Loads the Bitcoin Fear/Greed (daily) dataset.  \n4. Joins each trade with the *same-day* sentiment classification.  \n5. Produces core KPIs & exploratory charts.  \n6. Saves intermediate CSVs to `csv_files/` and figures to `outputs/`.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Environment & Packages\n\n> Colab tip: Set runtime to **Python 3** (default). No GPU needed.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If running on Colab, uncomment the next line to ensure all deps are available.\n# !pip -q install gdown pandas numpy matplotlib scipy statsmodels scikit-learn plotly\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, io, sys, json, math, warnings\nfrom datetime import datetime, timezone\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\n# Display options\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 200)\n\n# Paths (works on Colab and local)\nROOT = \".\"\nCSV_DIR = os.path.join(ROOT, \"csv_files\")\nOUT_DIR = os.path.join(ROOT, \"outputs\")\nos.makedirs(CSV_DIR, exist_ok=True)\nos.makedirs(OUT_DIR, exist_ok=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Download data from Google Drive\nReplace the file IDs if they change. Access must be set to \"Anyone with the link can view\".\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Google Drive file IDs (from assignment prompt)\nHYPERLIQUID_FILE_ID = \"1IAfLZwu6rJzyWKgBToqwSmmVYU6VbjVs\"\nFEAR_GREED_FILE_ID  = \"1PgQC0tO8XN-wqkNyghWc_-mnrYv_nhSf\"\n\n# Use gdown to download (works in Colab). If you're offline, upload manually via Colab sidebar.\ntry:\n    import gdown\n    hyperliquid_path = os.path.join(CSV_DIR, \"hyperliquid_trades.csv\")\n    feargreed_path   = os.path.join(CSV_DIR, \"fear_greed.csv\")\n    if not os.path.exists(hyperliquid_path):\n        gdown.download(id=HYPERLIQUID_FILE_ID, output=hyperliquid_path, quiet=False)\n    if not os.path.exists(feargreed_path):\n        gdown.download(id=FEAR_GREED_FILE_ID, output=feargreed_path, quiet=False)\n    print(\"Downloads completed.\")\nexcept Exception as e:\n    print(\"If gdown fails, upload the files manually to csv_files/ in Colab.\")\n    print(\"Error:\", e)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Load data\nThis loader is tolerant to CSV/JSON/Excel. It picks by extension; if you uploaded with a different name, update accordingly.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\n\ndef smart_read(path):\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(path)\n    ext = path.suffix.lower()\n    if ext in [\".csv\", \".txt\"]:\n        return pd.read_csv(path)\n    if ext in [\".json\"]:\n        return pd.read_json(path)\n    if ext in [\".xlsx\", \".xls\"]:\n        return pd.read_excel(path)\n    # Fallback assume CSV\n    return pd.read_csv(path)\n\nhyperliquid_raw = smart_read(os.path.join(CSV_DIR, \"hyperliquid_trades.csv\"))\nfeargreed_raw   = smart_read(os.path.join(CSV_DIR, \"fear_greed.csv\"))\n\nprint(\"Hyperliquid shape:\", hyperliquid_raw.shape)\nprint(\"Fear/Greed shape:\", feargreed_raw.shape)\ndisplay(hyperliquid_raw.head(3))\ndisplay(feargreed_raw.head(3))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Standardize columns & time\n- Normalize column names to snake_case.\n- Convert `time` to UTC-aware datetime if present (assumes UNIX ms or ISO).\n- Add a `trade_date` column (UTC date) to join with daily sentiment.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def to_snake(s):\n    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n\nhyper = hyperliquid_raw.copy()\nhyper.columns = [to_snake(c) for c in hyper.columns]\n\n# time parsing\ndef parse_time(x):\n    # try int (ms) -> datetime\n    try:\n        xi = int(x)\n        # Heuristic: treat as ms if > 10**12\n        if xi > 10**12:\n            return datetime.utcfromtimestamp(xi/1000).replace(tzinfo=timezone.utc)\n        return datetime.utcfromtimestamp(xi).replace(tzinfo=timezone.utc)\n    except Exception:\n        # try ISO\n        try:\n            dt = pd.to_datetime(x, utc=True)\n            if isinstance(dt, pd.Timestamp):\n                return dt.to_pydatetime()\n            return dt\n        except Exception:\n            return pd.NaT\n\nif \"time\" in hyper.columns:\n    hyper[\"time_utc\"] = hyper[\"time\"].apply(parse_time)\nelse:\n    # fallback: try 'timestamp' or similar\n    cand = [c for c in hyper.columns if \"time\" in c or \"stamp\" in c]\n    if cand:\n        hyper[\"time_utc\"] = hyper[cand[0]].apply(parse_time)\n    else:\n        hyper[\"time_utc\"] = pd.NaT\n\nhyper[\"trade_date\"] = pd.to_datetime(hyper[\"time_utc\"]).dt.floor(\"D\")\n\n# Standardize sentiment columns\nfg = feargreed_raw.copy()\nfg.columns = [to_snake(c) for c in fg.columns]\n# Expected: 'date', 'classification' (Fear/Greed)\nif \"date\" in fg.columns:\n    fg[\"date\"] = pd.to_datetime(fg[\"date\"], utc=True).dt.floor(\"D\")\nelse:\n    # try alternate names\n    cand = [c for c in fg.columns if \"date\" in c]\n    if cand:\n        fg[\"date\"] = pd.to_datetime(fg[cand[0]], utc=True).dt.floor(\"D\")\n    else:\n        raise ValueError(\"No date column found in fear/greed dataset\")\n\n# Keep only what we need\nfg = fg[[\"date\"] + ([c for c in fg.columns if \"class\" in c] or [\"classification\"])].copy()\nif \"classification\" not in fg.columns:\n    # try to find classification-like column (fear/greed)\n    cand = [c for c in fg.columns if \"class\" in c or \"sentiment\" in c]\n    if cand:\n        fg.rename(columns={cand[0]: \"classification\"}, inplace=True)\n    else:\n        raise ValueError(\"No 'classification' column found in sentiment file\")\n\nfg[\"classification\"] = fg[\"classification\"].astype(str).str.strip().str.title()  # 'Fear'/'Greed'\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Join trades to sentiment (by date)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = hyper.merge(fg, left_on=\"trade_date\", right_on=\"date\", how=\"left\", validate=\"m:1\")\ndf.rename(columns={\"classification\": \"sentiment\"}, inplace=True)\nprint(\"Merged shape:\", df.shape)\ndisplay(df.head(5))\n\n# Save merged for reuse\nmerged_path = os.path.join(CSV_DIR, \"trades_with_sentiment.csv\")\ndf.to_csv(merged_path, index=False)\nprint(\"Saved:\", merged_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Core KPIs\n- **Win rate**: share of trades with `closedPnL > 0`  \n- **Avg PnL per trade** and per account  \n- **Leverage usage**: mean/median by sentiment  \n- **Position sizing**: mean/median `size` by sentiment and side  \n- **Risk-adjusted** (rough): PnL per unit notional, and rolling Sharpe proxies\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Normalize key columns if present\nfor col in [\"closedpnl\", \"closed_pnl\"]:\n    if col in df.columns:\n        df[\"closed_pnl\"] = pd.to_numeric(df[col], errors=\"coerce\")\n        break\nif \"closed_pnl\" not in df.columns:\n    df[\"closed_pnl\"] = np.nan\n\nfor col in [\"size\"]:\n    if col not in df.columns:\n        df[col] = np.nan\n\nif \"leverage\" not in df.columns:\n    df[\"leverage\"] = np.nan\n\nif \"side\" not in df.columns:\n    df[\"side\"] = \"unknown\"\n\ndf[\"win\"] = (df[\"closed_pnl\"] > 0).astype(int)\n\n# KPI by sentiment\nkpi_sent = df.groupby(\"sentiment\").agg(\n    n_trades=(\"win\", \"count\"),\n    win_rate=(\"win\", \"mean\"),\n    avg_pnl=(\"closed_pnl\", \"mean\"),\n    median_pnl=(\"closed_pnl\", \"median\"),\n    avg_size=(\"size\", \"mean\"),\n    med_size=(\"size\", \"median\"),\n    avg_lev=(\"leverage\", \"mean\"),\n    med_lev=(\"leverage\", \"median\"),\n).reset_index()\n\ndisplay(kpi_sent)\n\nkpi_path = os.path.join(CSV_DIR, \"kpi_by_sentiment.csv\")\nkpi_sent.to_csv(kpi_path, index=False)\nprint(\"Saved:\", kpi_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Visualization snippets\nRun these cells to produce key plots in `outputs/`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6a) Win rate by sentiment\nfig = plt.figure()\nax = fig.add_subplot(111)\nkpi_sent.plot(x=\"sentiment\", y=\"win_rate\", kind=\"bar\", ax=ax, legend=False)\nax.set_title(\"Win Rate by Sentiment\")\nax.set_ylabel(\"Win Rate\")\nfig.tight_layout()\nfig_path = os.path.join(OUT_DIR, \"winrate_by_sentiment.png\")\nfig.savefig(fig_path, dpi=160)\nprint(\"Saved:\", fig_path)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6b) PnL distribution by sentiment (basic)\nfig = plt.figure()\nax = fig.add_subplot(111)\nfor snt in [\"Fear\", \"Greed\"]:\n    subset = df[df[\"sentiment\"]==snt][\"closed_pnl\"].dropna()\n    subset.plot(kind=\"hist\", bins=50, alpha=0.5, ax=ax, label=snt)\nax.set_title(\"Closed PnL Distribution by Sentiment\")\nax.set_xlabel(\"Closed PnL\")\nax.legend()\nfig.tight_layout()\nfig_path = os.path.join(OUT_DIR, \"pnl_hist_by_sentiment.png\")\nfig.savefig(fig_path, dpi=160)\nprint(\"Saved:\", fig_path)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6c) Leverage usage by sentiment\nfig = plt.figure()\nax = fig.add_subplot(111)\nkpi_sent.plot(x=\"sentiment\", y=\"avg_lev\", kind=\"bar\", ax=ax, legend=False)\nax.set_title(\"Average Leverage by Sentiment\")\nax.set_ylabel(\"Average Leverage\")\nfig.tight_layout()\nfig_path = os.path.join(OUT_DIR, \"avg_leverage_by_sentiment.png\")\nfig.savefig(fig_path, dpi=160)\nprint(\"Saved:\", fig_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Save per-account KPIs (useful for clustering later)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "acct_kpis = df.groupby([\"account\",\"sentiment\"]).agg(\n    n=(\"win\",\"count\"),\n    win_rate=(\"win\",\"mean\"),\n    avg_pnl=(\"closed_pnl\",\"mean\"),\n    avg_size=(\"size\",\"mean\"),\n    avg_lev=(\"leverage\",\"mean\")\n).reset_index()\n\nacct_kpis_pivot = acct_kpis.pivot_table(index=\"account\", columns=\"sentiment\",\n                                        values=[\"n\",\"win_rate\",\"avg_pnl\",\"avg_size\",\"avg_lev\"])\nacct_kpis_pivot.columns = [f\"{a}_{b}\".lower() for a,b in acct_kpis_pivot.columns.to_flat_index()]\nacct_kpis_pivot = acct_kpis_pivot.fillna(0.0)\n\nacct_path = os.path.join(CSV_DIR, \"account_kpis_wide.csv\")\nacct_kpis_pivot.to_csv(acct_path)\nprint(\"Saved:\", acct_path)\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_1.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}